<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Machine Learning | MAS109 - An Introduction to Data Science</title>
  <meta name="description" content="Lecture notes for MAS109" />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Machine Learning | MAS109 - An Introduction to Data Science" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for MAS109" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Machine Learning | MAS109 - An Introduction to Data Science" />
  
  <meta name="twitter:description" content="Lecture notes for MAS109" />
  

<meta name="author" content="Dr Jill Johnson" />


<meta name="date" content="2025-02-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="exploratory-data-analysis-using-r.html"/>
<link rel="next" href="populations-samples-and-statistical-models.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MAS109 - Introduction to Data Science</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About these notes</a></li>
<li class="chapter" data-level="1" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html"><i class="fa fa-check"></i><b>1</b> Exploratory Data Analysis using R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#case-study-what-makes-a-country-good-at-maths"><i class="fa fa-check"></i><b>1.1</b> Case study: what makes a country good at maths?</a></li>
<li class="chapter" data-level="1.2" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#the-tidyverse"><i class="fa fa-check"></i><b>1.2</b> The “Tidyverse”</a></li>
<li class="chapter" data-level="1.3" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#importing-data-into-r-csv-and-.xlsx-files"><i class="fa fa-check"></i><b>1.3</b> Importing data into R: <code>csv</code> and <code>.xlsx</code> files</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#importing-excel-.xlsx-files"><i class="fa fa-check"></i><b>1.3.1</b> Importing Excel <code>.xlsx</code> files</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#data-frames-and-tibbles-in-r"><i class="fa fa-check"></i><b>1.4</b> Data frames and tibbles in R</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#ordering-the-rows-by-a-variable-with-the-arrange-command"><i class="fa fa-check"></i><b>1.4.1</b> Ordering the rows by a variable with the <code>arrange()</code> command</a></li>
<li class="chapter" data-level="1.4.2" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#selecting-rows-with-the-filter-command"><i class="fa fa-check"></i><b>1.4.2</b> Selecting rows with the <code>filter()</code> command</a></li>
<li class="chapter" data-level="1.4.3" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#viewing-and-extracting-data-from-a-column"><i class="fa fa-check"></i><b>1.4.3</b> Viewing and extracting data from a column</a></li>
<li class="chapter" data-level="1.4.4" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#creating-new-columns-in-a-data-frame-with-the-mutate-command"><i class="fa fa-check"></i><b>1.4.4</b> Creating new columns in a data frame with the <code>mutate()</code> command</a></li>
<li class="chapter" data-level="1.4.5" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#chaining-commands-together-with-the-pipe-operator"><i class="fa fa-check"></i><b>1.4.5</b> Chaining commands together with the pipe operator <code>%&gt;%</code></a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#calculating-summary-statistics-with-the-summary-command"><i class="fa fa-check"></i><b>1.5</b> Calculating summary statistics with the <code>summary()</code> command</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#calculating-individual-summary-statistics"><i class="fa fa-check"></i><b>1.5.1</b> Calculating individual summary statistics</a></li>
<li class="chapter" data-level="1.5.2" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#calculating-other-quantilespercentiles"><i class="fa fa-check"></i><b>1.5.2</b> Calculating other quantiles/percentiles</a></li>
<li class="chapter" data-level="1.5.3" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#computing-summaries-per-group"><i class="fa fa-check"></i><b>1.5.3</b> Computing summaries per group</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#plotting-a-distribution-using-a-histogram"><i class="fa fa-check"></i><b>1.6</b> Plotting a distribution using a histogram</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#describing-the-shape-of-a-distribution-skewness"><i class="fa fa-check"></i><b>1.6.1</b> Describing the shape of a distribution: skewness</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#introducing-ggplot2"><i class="fa fa-check"></i><b>1.7</b> Introducing <code>ggplot2</code></a></li>
<li class="chapter" data-level="1.8" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#drawing-a-histogram-in-r"><i class="fa fa-check"></i><b>1.8</b> Drawing a histogram in R</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#customising-a-histogram-plot-in-r"><i class="fa fa-check"></i><b>1.8.1</b> Customising a histogram plot in R</a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#covariance-and-correlation"><i class="fa fa-check"></i><b>1.9</b> Covariance and correlation</a>
<ul>
<li class="chapter" data-level="1.9.1" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#calculating-a-covariance-in-r"><i class="fa fa-check"></i><b>1.9.1</b> Calculating a covariance in R</a></li>
<li class="chapter" data-level="1.9.2" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#pearsons-correlation-coefficient"><i class="fa fa-check"></i><b>1.9.2</b> Pearson’s correlation coefficient</a></li>
<li class="chapter" data-level="1.9.3" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#calculating-pearsons-correlation-coefficient-in-r"><i class="fa fa-check"></i><b>1.9.3</b> Calculating Pearson’s correlation coefficient in R</a></li>
<li class="chapter" data-level="1.9.4" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#spearmans-correlation-coefficient"><i class="fa fa-check"></i><b>1.9.4</b> Spearman’s correlation coefficient</a></li>
<li class="chapter" data-level="1.9.5" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#calculating-spearmans-correlation-coefficient-in-r"><i class="fa fa-check"></i><b>1.9.5</b> Calculating Spearman’s correlation coefficient in R</a></li>
<li class="chapter" data-level="1.9.6" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#interpreting-correlation-coefficients"><i class="fa fa-check"></i><b>1.9.6</b> Interpreting correlation coefficients</a></li>
<li class="chapter" data-level="1.9.7" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#correlations-for-the-maths-data-set"><i class="fa fa-check"></i><b>1.9.7</b> Correlations for the <code>maths</code> data set</a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#drawing-a-scatter-plot-in-r"><i class="fa fa-check"></i><b>1.10</b> Drawing a scatter plot in R</a>
<ul>
<li class="chapter" data-level="1.10.1" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#customising-a-scatter-plot-in-r"><i class="fa fa-check"></i><b>1.10.1</b> Customising a scatter plot in R</a></li>
<li class="chapter" data-level="1.10.2" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#adding-a-nonlinear-trend-to-a-scatter-plot-in-r"><i class="fa fa-check"></i><b>1.10.2</b> Adding a nonlinear trend to a scatter plot in R</a></li>
<li class="chapter" data-level="1.10.3" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#adding-a-linear-trend-to-a-scatter-plot-in-r"><i class="fa fa-check"></i><b>1.10.3</b> Adding a linear trend to a scatter plot in R</a></li>
</ul></li>
<li class="chapter" data-level="1.11" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#box-plots"><i class="fa fa-check"></i><b>1.11</b> Box plots</a></li>
<li class="chapter" data-level="1.12" data-path="exploratory-data-analysis-using-r.html"><a href="exploratory-data-analysis-using-r.html#scatter-plots-to-represent-three-variables"><i class="fa fa-check"></i><b>1.12</b> Scatter plots to represent three variables</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="machine-learning.html"><a href="machine-learning.html"><i class="fa fa-check"></i><b>2</b> Machine Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="machine-learning.html"><a href="machine-learning.html#can-we-teach-a-computer-to-identify-handwritten-digits"><i class="fa fa-check"></i><b>2.1</b> Can we teach a computer to identify handwritten digits?</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="machine-learning.html"><a href="machine-learning.html#step-1-converting-an-image-into-data"><i class="fa fa-check"></i><b>2.1.1</b> Step 1: converting an image into data</a></li>
<li class="chapter" data-level="2.1.2" data-path="machine-learning.html"><a href="machine-learning.html#step-2-assembling-the-training-data-set"><i class="fa fa-check"></i><b>2.1.2</b> Step 2: assembling the training data set</a></li>
<li class="chapter" data-level="2.1.3" data-path="machine-learning.html"><a href="machine-learning.html#step-3-an-algorithm-for-estimating-the-digit-in-a-new-image"><i class="fa fa-check"></i><b>2.1.3</b> Step 3: an algorithm for estimating the digit in a new image</a></li>
<li class="chapter" data-level="2.1.4" data-path="machine-learning.html"><a href="machine-learning.html#the-k-nearest-neighbour-algorithm-knn"><i class="fa fa-check"></i><b>2.1.4</b> The <span class="math inline">\(K\)</span> nearest neighbour algorithm (KNN)</a></li>
<li class="chapter" data-level="2.1.5" data-path="machine-learning.html"><a href="machine-learning.html#using-k-nearest-neighbours-in-r"><i class="fa fa-check"></i><b>2.1.5</b> Using <span class="math inline">\(K\)</span> nearest neighbours in R</a></li>
<li class="chapter" data-level="2.1.6" data-path="machine-learning.html"><a href="machine-learning.html#the-performance-of-the-algorithm"><i class="fa fa-check"></i><b>2.1.6</b> The performance of the algorithm</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="populations-samples-and-statistical-models.html"><a href="populations-samples-and-statistical-models.html"><i class="fa fa-check"></i><b>3</b> Populations, samples and statistical models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="populations-samples-and-statistical-models.html"><a href="populations-samples-and-statistical-models.html#statistical-models"><i class="fa fa-check"></i><b>3.1</b> Statistical models</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="populations-samples-and-statistical-models.html"><a href="populations-samples-and-statistical-models.html#objectives"><i class="fa fa-check"></i><b>3.1.1</b> Objectives</a></li>
<li class="chapter" data-level="3.1.2" data-path="populations-samples-and-statistical-models.html"><a href="populations-samples-and-statistical-models.html#comment-infinite-and-finite-populations"><i class="fa fa-check"></i><b>3.1.2</b> Comment: infinite and finite populations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="point-estimation.html"><a href="point-estimation.html"><i class="fa fa-check"></i><b>4</b> Point estimation</a>
<ul>
<li class="chapter" data-level="4.1" data-path="point-estimation.html"><a href="point-estimation.html#estimating-the-parameters-of-a-normal-distribution"><i class="fa fa-check"></i><b>4.1</b> Estimating the parameters of a normal distribution</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="point-estimation.html"><a href="point-estimation.html#problem-setup-and-notation"><i class="fa fa-check"></i><b>4.1.1</b> Problem setup and notation</a></li>
<li class="chapter" data-level="4.1.2" data-path="point-estimation.html"><a href="point-estimation.html#the-sample-mean-and-sample-variance"><i class="fa fa-check"></i><b>4.1.2</b> The sample mean and sample variance</a></li>
<li class="chapter" data-level="4.1.3" data-path="point-estimation.html"><a href="point-estimation.html#point-estimates-for-the-mean-and-variance"><i class="fa fa-check"></i><b>4.1.3</b> Point estimates for the mean and variance</a></li>
<li class="chapter" data-level="4.1.4" data-path="point-estimation.html"><a href="point-estimation.html#testing-the-method"><i class="fa fa-check"></i><b>4.1.4</b> Testing the method</a></li>
<li class="chapter" data-level="4.1.5" data-path="point-estimation.html"><a href="point-estimation.html#estimators-and-estimates"><i class="fa fa-check"></i><b>4.1.5</b> Estimators and estimates</a></li>
<li class="chapter" data-level="4.1.6" data-path="point-estimation.html"><a href="point-estimation.html#the-chi2-distribution"><i class="fa fa-check"></i><b>4.1.6</b> The <span class="math inline">\(\chi^2\)</span> distribution</a></li>
<li class="chapter" data-level="4.1.7" data-path="point-estimation.html"><a href="point-estimation.html#the-distribution-of-the-estimators"><i class="fa fa-check"></i><b>4.1.7</b> The distribution of the estimators</a></li>
<li class="chapter" data-level="4.1.8" data-path="point-estimation.html"><a href="point-estimation.html#unbiased-estimators"><i class="fa fa-check"></i><b>4.1.8</b> Unbiased estimators</a></li>
<li class="chapter" data-level="4.1.9" data-path="point-estimation.html"><a href="point-estimation.html#the-standard-error-of-an-estimator"><i class="fa fa-check"></i><b>4.1.9</b> The standard error of an estimator</a></li>
<li class="chapter" data-level="4.1.10" data-path="point-estimation.html"><a href="point-estimation.html#consistent-estimators"><i class="fa fa-check"></i><b>4.1.10</b> Consistent estimators</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="point-estimation.html"><a href="point-estimation.html#estimating-the-probability-parameter-in-a-binomial-distribution"><i class="fa fa-check"></i><b>4.2</b> Estimating the probability parameter in a Binomial distribution</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="interval-estimates-and-confidence-intervals.html"><a href="interval-estimates-and-confidence-intervals.html"><i class="fa fa-check"></i><b>5</b> Interval estimates and confidence intervals</a>
<ul>
<li class="chapter" data-level="5.1" data-path="interval-estimates-and-confidence-intervals.html"><a href="interval-estimates-and-confidence-intervals.html#the-student-t-distribution"><i class="fa fa-check"></i><b>5.1</b> The Student <span class="math inline">\(t\)</span> distribution</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="interval-estimates-and-confidence-intervals.html"><a href="interval-estimates-and-confidence-intervals.html#mean-and-variance-of-the-t-distribution"><i class="fa fa-check"></i><b>5.1.1</b> Mean and variance of the <span class="math inline">\(t\)</span>-distribution</a></li>
<li class="chapter" data-level="5.1.2" data-path="interval-estimates-and-confidence-intervals.html"><a href="interval-estimates-and-confidence-intervals.html#notation-quantilespercentiles-of-the-t-distribution"><i class="fa fa-check"></i><b>5.1.2</b> Notation: quantiles/percentiles of the <span class="math inline">\(t\)</span> distribution</a></li>
<li class="chapter" data-level="5.1.3" data-path="interval-estimates-and-confidence-intervals.html"><a href="interval-estimates-and-confidence-intervals.html#the-t-distribution-in-r."><i class="fa fa-check"></i><b>5.1.3</b> The <span class="math inline">\(t\)</span> distribution in R.</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="interval-estimates-and-confidence-intervals.html"><a href="interval-estimates-and-confidence-intervals.html#confidence-intervals-for-the-mean-and-the-variance-of-a-normal-distribution"><i class="fa fa-check"></i><b>5.2</b> Confidence intervals for the mean and the variance of a normal distribution</a></li>
<li class="chapter" data-level="5.3" data-path="interval-estimates-and-confidence-intervals.html"><a href="interval-estimates-and-confidence-intervals.html#confidence-interval-for-the-probability-parameter-in-a-binomial-distribution"><i class="fa fa-check"></i><b>5.3</b> Confidence interval for the probability parameter in a binomial distribution</a></li>
<li class="chapter" data-level="5.4" data-path="interval-estimates-and-confidence-intervals.html"><a href="interval-estimates-and-confidence-intervals.html#alpha-confidence-intervals"><i class="fa fa-check"></i><b>5.4</b> <span class="math inline">\(100(1-\alpha)\%\)</span> Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="hypothesis-testing-a-level-recap.html"><a href="hypothesis-testing-a-level-recap.html"><i class="fa fa-check"></i><b>6</b> Hypothesis testing: A-level recap</a>
<ul>
<li class="chapter" data-level="6.1" data-path="hypothesis-testing-a-level-recap.html"><a href="hypothesis-testing-a-level-recap.html#hypothesis-testing-with-the-neyman-pearson-approach"><i class="fa fa-check"></i><b>6.1</b> Hypothesis testing with the Neyman-Pearson approach</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="hypothesis-testing-a-level-recap.html"><a href="hypothesis-testing-a-level-recap.html#one-sided-and-two-sided-alternative-hypotheses"><i class="fa fa-check"></i><b>6.1.1</b> One-sided and two-sided alternative hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="hypothesis-testing-a-level-recap.html"><a href="hypothesis-testing-a-level-recap.html#fishers-p-value-method"><i class="fa fa-check"></i><b>6.2</b> Fisher’s <span class="math inline">\(p\)</span>-value method</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="hypothesis-testing-a-level-recap.html"><a href="hypothesis-testing-a-level-recap.html#what-counts-as-a-small-p-value"><i class="fa fa-check"></i><b>6.2.1</b> What counts as a small <span class="math inline">\(p\)</span>-value?</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="hypothesis-testing-a-level-recap.html"><a href="hypothesis-testing-a-level-recap.html#relationship-between-the-neyman-pearson-and-p-value-methods"><i class="fa fa-check"></i><b>6.3</b> Relationship between the Neyman-Pearson and <span class="math inline">\(p\)</span>-value methods</a></li>
<li class="chapter" data-level="6.4" data-path="hypothesis-testing-a-level-recap.html"><a href="hypothesis-testing-a-level-recap.html#which-hypothesis-test-do-i-use-for"><i class="fa fa-check"></i><b>6.4</b> Which hypothesis test do I use for…?</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="hypothesis-testing-comparing-two-population-means.html"><a href="hypothesis-testing-comparing-two-population-means.html"><i class="fa fa-check"></i><b>7</b> Hypothesis testing: comparing two population means</a>
<ul>
<li class="chapter" data-level="7.1" data-path="hypothesis-testing-comparing-two-population-means.html"><a href="hypothesis-testing-comparing-two-population-means.html#example-can-imagining-eating-food-make-you-eat-less"><i class="fa fa-check"></i><b>7.1</b> Example: can imagining eating food make you eat less?</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="hypothesis-testing-comparing-two-population-means.html"><a href="hypothesis-testing-comparing-two-population-means.html#the-hypotheses"><i class="fa fa-check"></i><b>7.1.1</b> The hypotheses</a></li>
<li class="chapter" data-level="7.1.2" data-path="hypothesis-testing-comparing-two-population-means.html"><a href="hypothesis-testing-comparing-two-population-means.html#a-test-statistic"><i class="fa fa-check"></i><b>7.1.2</b> A test statistic</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="hypothesis-testing-comparing-two-population-means.html"><a href="hypothesis-testing-comparing-two-population-means.html#hypothesis-testing-using-simulation"><i class="fa fa-check"></i><b>7.2</b> Hypothesis testing using simulation</a></li>
<li class="chapter" data-level="7.3" data-path="hypothesis-testing-comparing-two-population-means.html"><a href="hypothesis-testing-comparing-two-population-means.html#the-two-sample-t-test"><i class="fa fa-check"></i><b>7.3</b> The two-sample <span class="math inline">\(t\)</span> test</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="hypothesis-testing-comparing-two-population-means.html"><a href="hypothesis-testing-comparing-two-population-means.html#the-two-sample-t-test-with-the-neyman-pearson-method"><i class="fa fa-check"></i><b>7.3.1</b> The two-sample <span class="math inline">\(t\)</span>-test with the Neyman-Pearson method</a></li>
<li class="chapter" data-level="7.3.2" data-path="hypothesis-testing-comparing-two-population-means.html"><a href="hypothesis-testing-comparing-two-population-means.html#an-illustrated-guide"><i class="fa fa-check"></i><b>7.3.2</b> An illustrated guide</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="hypothesis-testing-comparing-two-population-means.html"><a href="hypothesis-testing-comparing-two-population-means.html#confidence-interval-for-the-difference-between-two-means"><i class="fa fa-check"></i><b>7.4</b> Confidence interval for the difference between two means</a></li>
<li class="chapter" data-level="7.5" data-path="hypothesis-testing-comparing-two-population-means.html"><a href="hypothesis-testing-comparing-two-population-means.html#equivalence-of-confidence-intervals-and-neyman-pearson-testing"><i class="fa fa-check"></i><b>7.5</b> Equivalence of confidence intervals and Neyman-Pearson testing</a></li>
<li class="chapter" data-level="7.6" data-path="hypothesis-testing-comparing-two-population-means.html"><a href="hypothesis-testing-comparing-two-population-means.html#the-two-sample-t-test-in-r"><i class="fa fa-check"></i><b>7.6</b> The two-sample <span class="math inline">\(t\)</span> test in R</a>
<ul>
<li class="chapter" data-level="7.6.1" data-path="hypothesis-testing-comparing-two-population-means.html"><a href="hypothesis-testing-comparing-two-population-means.html#t-tests-with-data-frames-in-r"><i class="fa fa-check"></i><b>7.6.1</b> <span class="math inline">\(t\)</span>-tests with data frames in R</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="hypothesis-testing-comparing-two-population-means.html"><a href="hypothesis-testing-comparing-two-population-means.html#examples"><i class="fa fa-check"></i><b>7.7</b> Examples</a>
<ul>
<li class="chapter" data-level="7.7.1" data-path="hypothesis-testing-comparing-two-population-means.html"><a href="hypothesis-testing-comparing-two-population-means.html#using-the-p-value-method"><i class="fa fa-check"></i><b>7.7.1</b> Using the <span class="math inline">\(p\)</span>-value method</a></li>
<li class="chapter" data-level="7.7.2" data-path="hypothesis-testing-comparing-two-population-means.html"><a href="hypothesis-testing-comparing-two-population-means.html#using-neyman-pearson-testing"><i class="fa fa-check"></i><b>7.7.2</b> Using Neyman-Pearson testing</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="hypothesis-testing-comparing-two-proportions.html"><a href="hypothesis-testing-comparing-two-proportions.html"><i class="fa fa-check"></i><b>8</b> Hypothesis testing: comparing two proportions</a>
<ul>
<li class="chapter" data-level="8.1" data-path="hypothesis-testing-comparing-two-proportions.html"><a href="hypothesis-testing-comparing-two-proportions.html#example-an-investigation-into-gender-bias"><i class="fa fa-check"></i><b>8.1</b> Example: an investigation into gender bias</a></li>
<li class="chapter" data-level="8.2" data-path="hypothesis-testing-comparing-two-proportions.html"><a href="hypothesis-testing-comparing-two-proportions.html#comparing-two-binomial-proportions"><i class="fa fa-check"></i><b>8.2</b> Comparing two binomial proportions</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="hypothesis-testing-comparing-two-proportions.html"><a href="hypothesis-testing-comparing-two-proportions.html#a-simulation-method"><i class="fa fa-check"></i><b>8.2.1</b> A simulation method</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="hypothesis-testing-comparing-two-proportions.html"><a href="hypothesis-testing-comparing-two-proportions.html#an-analytical-method"><i class="fa fa-check"></i><b>8.3</b> An analytical method</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="hypothesis-testing-comparing-two-proportions.html"><a href="hypothesis-testing-comparing-two-proportions.html#the-analytical-method-a-summary"><i class="fa fa-check"></i><b>8.3.1</b> The analytical method: a summary</a></li>
<li class="chapter" data-level="8.3.2" data-path="hypothesis-testing-comparing-two-proportions.html"><a href="hypothesis-testing-comparing-two-proportions.html#conclusion"><i class="fa fa-check"></i><b>8.3.2</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="hypothesis-testing-comparing-two-proportions.html"><a href="hypothesis-testing-comparing-two-proportions.html#confidence-intervals-to-measure-the-difference"><i class="fa fa-check"></i><b>8.4</b> Confidence intervals to measure the difference</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="sample-size-and-power-for-a-neyman-pearson-hypothesis-test.html"><a href="sample-size-and-power-for-a-neyman-pearson-hypothesis-test.html"><i class="fa fa-check"></i><b>9</b> Sample size and power for a Neyman-Pearson hypothesis test</a>
<ul>
<li class="chapter" data-level="9.1" data-path="sample-size-and-power-for-a-neyman-pearson-hypothesis-test.html"><a href="sample-size-and-power-for-a-neyman-pearson-hypothesis-test.html#gender-bias-example-re-visited"><i class="fa fa-check"></i><b>9.1</b> Gender bias example re-visited</a></li>
<li class="chapter" data-level="9.2" data-path="sample-size-and-power-for-a-neyman-pearson-hypothesis-test.html"><a href="sample-size-and-power-for-a-neyman-pearson-hypothesis-test.html#the-power-of-a-hypothesis-test"><i class="fa fa-check"></i><b>9.2</b> The power of a hypothesis test</a></li>
<li class="chapter" data-level="9.3" data-path="sample-size-and-power-for-a-neyman-pearson-hypothesis-test.html"><a href="sample-size-and-power-for-a-neyman-pearson-hypothesis-test.html#an-analytical-approach"><i class="fa fa-check"></i><b>9.3</b> An analytical approach</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chi2-tests-for-contingency-tables.html"><a href="chi2-tests-for-contingency-tables.html"><i class="fa fa-check"></i><b>10</b> <span class="math inline">\(\chi^2\)</span> tests for contingency tables</a>
<ul>
<li class="chapter" data-level="10.1" data-path="chi2-tests-for-contingency-tables.html"><a href="chi2-tests-for-contingency-tables.html#example-customer-ratings-of-restaurants"><i class="fa fa-check"></i><b>10.1</b> Example: customer ratings of restaurants</a></li>
<li class="chapter" data-level="10.2" data-path="chi2-tests-for-contingency-tables.html"><a href="chi2-tests-for-contingency-tables.html#a-model-and-hypotheses"><i class="fa fa-check"></i><b>10.2</b> A model and hypotheses</a></li>
<li class="chapter" data-level="10.3" data-path="chi2-tests-for-contingency-tables.html"><a href="chi2-tests-for-contingency-tables.html#a-test-statistic-1"><i class="fa fa-check"></i><b>10.3</b> A test statistic</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="chi2-tests-for-contingency-tables.html"><a href="chi2-tests-for-contingency-tables.html#the-formula-for-the-expected-counts"><i class="fa fa-check"></i><b>10.3.1</b> The formula for the expected counts</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="chi2-tests-for-contingency-tables.html"><a href="chi2-tests-for-contingency-tables.html#computing-the-test-statistic-for-the-observed-data"><i class="fa fa-check"></i><b>10.4</b> Computing the test statistic for the observed data</a></li>
<li class="chapter" data-level="10.5" data-path="chi2-tests-for-contingency-tables.html"><a href="chi2-tests-for-contingency-tables.html#a-simulation-method-1"><i class="fa fa-check"></i><b>10.5</b> A simulation method</a></li>
<li class="chapter" data-level="10.6" data-path="chi2-tests-for-contingency-tables.html"><a href="chi2-tests-for-contingency-tables.html#an-analytical-method-1"><i class="fa fa-check"></i><b>10.6</b> An analytical method</a>
<ul>
<li class="chapter" data-level="10.6.1" data-path="chi2-tests-for-contingency-tables.html"><a href="chi2-tests-for-contingency-tables.html#chi2-tests-in-r"><i class="fa fa-check"></i><b>10.6.1</b> <span class="math inline">\(\chi^2\)</span> tests in R</a></li>
<li class="chapter" data-level="10.6.2" data-path="chi2-tests-for-contingency-tables.html"><a href="chi2-tests-for-contingency-tables.html#row-homogeneity-and-independence"><i class="fa fa-check"></i><b>10.6.2</b> Row homogeneity and independence</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="chi2-tests-for-contingency-tables.html"><a href="chi2-tests-for-contingency-tables.html#exercise"><i class="fa fa-check"></i><b>10.7</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="index-of-definitions-and-examples.html"><a href="index-of-definitions-and-examples.html"><i class="fa fa-check"></i><b>11</b> Index of definitions and examples</a>
<ul>
<li class="chapter" data-level="11.1" data-path="index-of-definitions-and-examples.html"><a href="index-of-definitions-and-examples.html#definitions"><i class="fa fa-check"></i><b>11.1</b> Definitions</a></li>
<li class="chapter" data-level="11.2" data-path="index-of-definitions-and-examples.html"><a href="index-of-definitions-and-examples.html#examples-1"><i class="fa fa-check"></i><b>11.2</b> Examples</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MAS109 - An Introduction to Data Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machine-learning" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Machine Learning<a href="machine-learning.html#machine-learning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The term “Machine Learning” was used in a paper by Arthur Samuel in 1959<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>, on how a computer could learn to play a better game of checkers. In general machine learning involves getting a computer to ‘learn from experience’, so that it can perform a particular task in a new situation without being programmed what to do. Machine learning underpins technology described as “artificial intelligence”.</p>
<p>Some machine learning problems involve getting computers to recognise speech, read handwriting, and identify objects in images. More recently, machine learning methods have been used to analyse very large and complex data sets, where the scale of the problem is too large for humans to manage (e.g. recommending products to millions of individual customers, based on vast databases of customer purchases).</p>
<p>This chapter will give a short introduction, where we will look at a single problem and method, and we will consider the role of data analysis within machine learning.</p>
<div id="can-we-teach-a-computer-to-identify-handwritten-digits" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Can we teach a computer to identify handwritten digits?<a href="machine-learning.html#can-we-teach-a-computer-to-identify-handwritten-digits" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Below are some scanned images of handwritten digits. The images are from the (well-known) “MNIST” data set, hosted on <a href="http://yann.lecun.com/exdb/mnist/">Yann Lecun’s website</a>.</p>
<p><em>We</em> can see what the digits are without too much difficulty, but can we get a computer to recognise the digits?</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:MNIST"></span>
<img src="MAS109-Data-Science_files/figure-html/MNIST-1.png" alt="Scanned images of hand-written digits (not all written by the same person). We can easily recognise what the digits are: could a computer do so?" width="384" />
<p class="caption">
Figure 2.1: Scanned images of hand-written digits (not all written by the same person). We can easily recognise what the digits are: could a computer do so?
</p>
</div>
<p>The key idea is to make this a <em>data analysis</em> problem. The steps are as follows.</p>
<ol style="list-style-type: decimal">
<li>Convert the objects we want the computer to recognise (the scanned images) into numerical data: find a way to represent each image with a set of numbers. The computer will need to be able to do this conversion automatically, <em>without</em> knowing what each digit in a scanned image actually is.</li>
<li>Construct a <strong>training data set</strong>: a (large) data set of example handwritten digits, all converted into numerical data. In addition, we <em>also</em> tell the computer what each handwritten digit is: “The first image in the data set is the number 5, the second image in the number 0,” and so on.</li>
<li>Construct a statistical model or algorithm using the training data set, that given an image in its converted numerical form, can estimate what the handwritten digit is.</li>
</ol>
<div class="rmdnote">
<p>
In this module, steps 1 and 2 will always be done for you; you will
only need one method for doing step 3.
</p>
</div>
<div id="step-1-converting-an-image-into-data" class="section level3 hasAnchor" number="2.1.1">
<h3><span class="header-section-number">2.1.1</span> Step 1: converting an image into data<a href="machine-learning.html#step-1-converting-an-image-into-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>(The images first need to be scaled to the same size, with the handwriting approximately in the centre of the image. This step has been done for us.)</p>
<p>Here is one example image:</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-61"></span>
<img src="MAS109-Data-Science_files/figure-html/unnamed-chunk-61-1.png" alt="A single scanned image of a handwritten digit. (The top left corner image from Figure 2.1.) Zooming in, we can see that the image is made up of shaded blocks." width="480" />
<p class="caption">
Figure 2.2: A single scanned image of a handwritten digit. (The top left corner image from Figure <a href="machine-learning.html#fig:MNIST">2.1</a>.) Zooming in, we can see that the image is made up of shaded blocks.
</p>
</div>
<p>The image is made up of pixels (shaded dots), arranged on a 28x28 grid (these are low resolution images!) The shading of each pixel can be represented numerically, on a scale of 0 (white) to 255 (black).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:training1"></span>
<img src="MAS109-Data-Science_files/figure-html/training1-1.png" alt="This is how the computer 'sees' the image, as a grid of pixel shades (here with higher numbers representing darker shades). We can use these numbers to represent the image in numerical form." width="480" />
<p class="caption">
Figure 2.3: This is how the computer ‘sees’ the image, as a grid of pixel shades (here with higher numbers representing darker shades). We can use these numbers to represent the image in numerical form.
</p>
</div>
<p>We can now represent the image by a vector <span class="math inline">\(\mathbf x\)</span> with <span class="math inline">\(28^2 = 784\)</span> elements:
<span class="math display">\[\mathbf x = (x_1,x_2,\ldots,x_{784}),\]</span> taking one row at a time from the above image. Starting with the top row, the vector would look like this:
<span class="math display">\[
\mathbf x=(0,0,0,\ldots,0,0, 0,3, 18, 18, 18, 126, 136,\ldots, 135, 132, 16, 0, 0, \ldots, 0, 0, 0)
\]</span></p>
</div>
<div id="step-2-assembling-the-training-data-set" class="section level3 hasAnchor" number="2.1.2">
<h3><span class="header-section-number">2.1.2</span> Step 2: assembling the training data set<a href="machine-learning.html#step-2-assembling-the-training-data-set" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now assemble a training data set of 60,000 images. The <span class="math inline">\(i\)</span>-th image in the data set is represented by the vector <span class="math inline">\(\mathbf x_i\)</span>, where
<span class="math display">\[
\mathbf x_i = (x_{i,1},x_{i,2},\ldots,x_{i,784})
\]</span>
We define <span class="math inline">\(y_i\)</span> to be the <strong>class label</strong> for the <span class="math inline">\(i\)</span>-th image. In this case, the class label will be a number from 0-9, that says what the hand written digit is. For the training data set, the class labels are simply given to us: we don’t need to work out what they should be. We write the training data set in the form <span class="math display">\[(\mathbf x_1,y_1), (\mathbf x_2,y_2),\ldots, (\mathbf x_{60,000}, y_{60,000}).\]</span></p>
<p>If <span class="math inline">\((\mathbf x_1, y_1)\)</span> corresponds to the first image in the training data set, then <span class="math inline">\(\mathbf x_1\)</span> is the vector with elements given by the numbers in Figure <a href="machine-learning.html#fig:training1">2.3</a>, and <span class="math inline">\(y_1=5\)</span>: the image is a hand-written number 5.</p>
<div id="setting-up-the-data-in-r." class="section level4 hasAnchor" number="2.1.2.1">
<h4><span class="header-section-number">2.1.2.1</span> Setting up the data in R.<a href="machine-learning.html#setting-up-the-data-in-r." class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If you want to get the data for yourself, I suggest you download the data in csv format: csv files <code>mnist_train.csv</code> and <code>mnist_test.csv</code> can be downloaded from <a href="https://pjreddie.com/projects/mnist-in-csv/">this site maintained by Joseph Redmon</a>. Once you have downloaded them, assuming the files are in your working directory, use the commands</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="machine-learning.html#cb90-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb90-2"><a href="machine-learning.html#cb90-2" tabindex="-1"></a>training_set <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;mnist_train.csv&quot;</span>, <span class="at">col_names =</span> <span class="cn">FALSE</span>)</span>
<span id="cb90-3"><a href="machine-learning.html#cb90-3" tabindex="-1"></a>test_set <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;mnist_test.csv&quot;</span>, <span class="at">col_names =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>For the training data, the class labels <span class="math inline">\(y_1,y_2,\ldots,y_{60000}\)</span> are stored in the first column of <code>training_set</code>. The image vector <span class="math inline">\(x_i\)</span> is stored in row <span class="math inline">\(i\)</span>, columns 2 to 725. As well as the training data, we have another data set know as the “test data”, which are stored are stored similarly in <code>test_set</code>.</p>
</div>
</div>
<div id="step-3-an-algorithm-for-estimating-the-digit-in-a-new-image" class="section level3 hasAnchor" number="2.1.3">
<h3><span class="header-section-number">2.1.3</span> Step 3: an algorithm for estimating the digit in a new image<a href="machine-learning.html#step-3-an-algorithm-for-estimating-the-digit-in-a-new-image" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have a separate <strong>test data set</strong> made up of 10,000 images. We will represent these by the vectors
<span class="math display">\[
\tilde{\mathbf  x}_1,\tilde{\mathbf x}_2,\ldots,\tilde{\mathbf x}_{10,000}
\]</span>
The first image in our test data set and its numerical representation <span class="math inline">\(\tilde{\mathbf x}_1\)</span> are shown below.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-63"></span>
<img src="MAS109-Data-Science_files/figure-html/unnamed-chunk-63-1.png" alt="The first image in the test data set, represented by a vector $\tilde{\mathbf x}_1$, with elements made up of the numbers on the right hand side. How can a computer use the training data to tell that this is a number '7'?" width="864" />
<p class="caption">
Figure 2.4: The first image in the test data set, represented by a vector <span class="math inline">\(\tilde{\mathbf x}_1\)</span>, with elements made up of the numbers on the right hand side. How can a computer use the training data to tell that this is a number ‘7’?
</p>
</div>
<p>How to get the computer to recognise that this is a number 7? There are lots of algorithms we could try (and, in general, many statistical models can be used for machine learning). Here, we will use a simple one, known as “<span class="math inline">\(K\)</span> nearest neighbours”.</p>
</div>
<div id="the-k-nearest-neighbour-algorithm-knn" class="section level3 hasAnchor" number="2.1.4">
<h3><span class="header-section-number">2.1.4</span> The <span class="math inline">\(K\)</span> nearest neighbour algorithm (KNN)<a href="machine-learning.html#the-k-nearest-neighbour-algorithm-knn" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the <span class="math inline">\(K\)</span> nearest neighbours method, the computer will decide what number an image is by looking for similar images in the training data. It can then use the known class labels in the training images to estimate what the new test image is. Writing out the vector</p>
<p><span class="math display">\[
\tilde{\mathbf x}_1 = (\tilde{x}_{1,1},\tilde{x}_{1,2},\ldots,\tilde{x}_{1,784})
\]</span></p>
<p>we will measure similarity using the (square of) the Euclidean distance between <span class="math inline">\(\tilde{\mathbf x}_1\)</span> and each training image <span class="math inline">\(\mathbf x_i\)</span>. We define
<span class="math display">\[
d(\tilde{\mathbf x}_1, \mathbf x_i) := \sum_{j=1}^{784}(\tilde{x}_{1, j} - x_{i,j})^2,
\]</span>
so the smaller the distance, the more similar the images are. The computer will decide what digit <span class="math inline">\(\tilde{\mathbf x}_1\)</span> is as follows:</p>
<ol style="list-style-type: decimal">
<li>Compute <span class="math inline">\(d(\tilde{\mathbf x}_1, \mathbf x_i)\)</span> for <span class="math inline">\(i=1,2,\ldots,60000\)</span></li>
<li>Find the nearest neighbour: look for smallest distance out of
<span class="math display">\[d(\tilde{\mathbf x}_1, \mathbf x_1),\quad d(\tilde{\mathbf x}_1, \mathbf x_2),\, \ldots\,,\,d(\tilde{\mathbf x}_1, \mathbf x_{60000})\]</span></li>
<li>If the nearest neighbour was image <span class="math inline">\(j\)</span> (the smallest distance was <span class="math inline">\(d(\tilde{\mathbf x}_1, \mathbf x_j))\)</span>, then estimate the digit to be <span class="math inline">\(y_j\)</span>: the known class label for image <span class="math inline">\(j\)</span>.</li>
</ol>
<p>As an example, we compute</p>
<p><span class="math display">\[\begin{align}
d(\tilde{\mathbf x}_1, \mathbf x_1) &amp;= \sum_{j=1}^{784}(\tilde{x}_{1, j} - x_{1,j})^2\\
&amp;= 5,739,837
\end{align}\]</span>
We view this below.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-64"></span>
<img src="MAS109-Data-Science_files/figure-html/unnamed-chunk-64-1.png" alt="Top row: the first test image and its numerical representation $\tilde{\mathbf x}_1$. Bottom row: the first image in the training data set and its numerical representation $\mathbf x_1$. To compute the similarity between the images, we square the differences between the numbers at the same corresponding positions in the right hand column, and sum." width="576" />
<p class="caption">
Figure 2.5: Top row: the first test image and its numerical representation <span class="math inline">\(\tilde{\mathbf x}_1\)</span>. Bottom row: the first image in the training data set and its numerical representation <span class="math inline">\(\mathbf x_1\)</span>. To compute the similarity between the images, we square the differences between the numbers at the same corresponding positions in the right hand column, and sum.
</p>
</div>
<p>We compute <span class="math inline">\(d(\tilde{\mathbf x}_1, \mathbf x_i)\)</span> for all 60000 images. The image most similar to <span class="math inline">\(\tilde{\mathbf x}_1\)</span> turns out to be image number 53844 in the training data set:
<span class="math display">\[\begin{align}
d(\tilde{\mathbf x}, \mathbf x_{53844}) &amp;= \sum_{j=1}^{784}(\tilde{x}_{1, j} - x_{53844,j})^2 \\
&amp;=457,766
\end{align}\]</span>
We view this below.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-65"></span>
<img src="MAS109-Data-Science_files/figure-html/unnamed-chunk-65-1.png" alt="Top row: the first test image and its numerical representation $\tilde{\mathbf x}_1$. Bottom row: image number 53844 in the training data set and its numerical representation $\mathbf x_{53844}$. To compute the similarity between the images, we square the differences between the numbers at the same corresponding positions in the right hand column, and sum. By this measure of similarity, image 53844 is closest to our test image." width="576" />
<p class="caption">
Figure 2.6: Top row: the first test image and its numerical representation <span class="math inline">\(\tilde{\mathbf x}_1\)</span>. Bottom row: image number 53844 in the training data set and its numerical representation <span class="math inline">\(\mathbf x_{53844}\)</span>. To compute the similarity between the images, we square the differences between the numbers at the same corresponding positions in the right hand column, and sum. By this measure of similarity, image 53844 is closest to our test image.
</p>
</div>
<p>So, the computer will look up the value of <span class="math inline">\(y_{53844}\)</span> in the training data set, which is recorded as 7, and so estimate the test image to be the number 7.</p>
<div id="the-k-in-k-nearest-neighbours" class="section level4 hasAnchor" number="2.1.4.1">
<h4><span class="header-section-number">2.1.4.1</span> The ‘<span class="math inline">\(K\)</span>’ in <span class="math inline">\(K\)</span> nearest neighbours<a href="machine-learning.html#the-k-in-k-nearest-neighbours" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We’ve actually used the simplest version of the KNN algorithm, where we look for the single nearest neighbour. An extension is to look for the <span class="math inline">\(K\)</span> nearest neighbours, and then choose the class label based on which which label occurs the most out of the <span class="math inline">\(K\)</span> nearest neighbours (so we have just used <span class="math inline">\(K=1\)</span> above). This may give better results, for example, if there is the odd ‘badly drawn’ image in the training data set, that looks like a different digit: it can be out-voted if we search for more nearest neighbours.</p>
</div>
</div>
<div id="using-k-nearest-neighbours-in-r" class="section level3 hasAnchor" number="2.1.5">
<h3><span class="header-section-number">2.1.5</span> Using <span class="math inline">\(K\)</span> nearest neighbours in R<a href="machine-learning.html#using-k-nearest-neighbours-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can use the function <code>knn()</code> from the package <code>class</code>.</p>
<div id="a-simple-example" class="section level4 hasAnchor" number="2.1.5.1">
<h4><span class="header-section-number">2.1.5.1</span> A simple example<a href="machine-learning.html#a-simple-example" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We’ll first do a simple example on a small data set, to make it easier to see how everything works.</p>
<p>(Ignore the following three commands, unless you want to try this on your own computer. These commands will make the data we are going to use for the example.)</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="machine-learning.html#cb91-1" tabindex="-1"></a>irisTrain <span class="ot">&lt;-</span> iris[<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">51</span>, <span class="dv">52</span>, <span class="dv">101</span>, <span class="dv">102</span>), <span class="dv">3</span><span class="sc">:</span><span class="dv">5</span>]</span>
<span id="cb91-2"><a href="machine-learning.html#cb91-2" tabindex="-1"></a>irisTest <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="at">flower =</span> <span class="fu">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>),</span>
<span id="cb91-3"><a href="machine-learning.html#cb91-3" tabindex="-1"></a>                  iris[<span class="fu">c</span>(<span class="dv">44</span>, <span class="dv">53</span>), <span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>]) </span>
<span id="cb91-4"><a href="machine-learning.html#cb91-4" tabindex="-1"></a><span class="fu">row.names</span>(irisTrain) <span class="ot">&lt;-</span> <span class="fu">row.names</span>(irisTest) <span class="ot">&lt;-</span> <span class="cn">NULL</span></span></code></pre></div>
<p>In our training data, a data frame called <code>irisTrain</code>, we have observations of the lengths and widths of petals for 6 flowers (species of iris). Each flower is one of three possible species: setosa, versicolor, or virginica.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="machine-learning.html#cb92-1" tabindex="-1"></a>irisTrain</span></code></pre></div>
<pre><code>##   Petal.Length Petal.Width    Species
## 1          1.4         0.2     setosa
## 2          1.7         0.4     setosa
## 3          4.7         1.4 versicolor
## 4          4.5         1.5 versicolor
## 5          6.0         2.5  virginica
## 6          5.1         1.9  virginica</code></pre>
<p>In our test data, <code>irisTest</code>, we have two iris flowers, labelled <code>A</code> and <code>B</code>, with measured petal lengths and widths: the aim is to predict the species of these two flowers.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="machine-learning.html#cb94-1" tabindex="-1"></a>irisTest</span></code></pre></div>
<pre><code>##   flower Petal.Length Petal.Width
## 1      A          1.6         0.6
## 2      B          4.9         1.5</code></pre>
<p>If we plot the training and test data together, we can see that the closest flower in the training data to flower A has species setosa, and the closet flower in the training data to flower A has species versicolor, so we would predict that flowers A and B are species setosa and versicolor respectively.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="machine-learning.html#cb96-1" tabindex="-1"></a><span class="fu">ggplot</span>(irisTrain, <span class="fu">aes</span>(<span class="at">x =</span> Petal.Length,</span>
<span id="cb96-2"><a href="machine-learning.html#cb96-2" tabindex="-1"></a>                      <span class="at">y =</span> Petal.Width))<span class="sc">+</span></span>
<span id="cb96-3"><a href="machine-learning.html#cb96-3" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">color =</span> Species)) <span class="sc">+</span></span>
<span id="cb96-4"><a href="machine-learning.html#cb96-4" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="at">x =</span> irisTest<span class="sc">$</span>Petal.Length,</span>
<span id="cb96-5"><a href="machine-learning.html#cb96-5" tabindex="-1"></a>           <span class="at">y =</span> irisTest<span class="sc">$</span>Petal.Width,</span>
<span id="cb96-6"><a href="machine-learning.html#cb96-6" tabindex="-1"></a>           <span class="at">label =</span> irisTest<span class="sc">$</span>flower)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:simpleIris"></span>
<img src="MAS109-Data-Science_files/figure-html/simpleIris-1.png" alt="The training data are shown as the 6 coloured points. The test data are marked as the two letters. The nearest neighbour to flower A has species setosa, and the nearest neighbour to flower B has species versicolor." width="384" />
<p class="caption">
Figure 2.7: The training data are shown as the 6 coloured points. The test data are marked as the two letters. The nearest neighbour to flower A has species setosa, and the nearest neighbour to flower B has species versicolor.
</p>
</div>
<p>To use the KNN algorithm in R, we use the function <code>knn()</code> from the <code>class</code> library. We specify three arguments:</p>
<ul>
<li><code>train</code>: the measurements in the training data, <strong>excluding</strong> the class labels (the <code>Species</code> column). We need to exclude column 3, which we can do as follows:</li>
</ul>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="machine-learning.html#cb97-1" tabindex="-1"></a>irisTrain[, <span class="sc">-</span><span class="dv">3</span>]</span></code></pre></div>
<pre><code>##   Petal.Length Petal.Width
## 1          1.4         0.2
## 2          1.7         0.4
## 3          4.7         1.4
## 4          4.5         1.5
## 5          6.0         2.5
## 6          5.1         1.9</code></pre>
<ul>
<li><code>test</code>: the measurements in the test data. We will need to exclude the <code>flower</code> labels. These labels are also in the first column, so we do</li>
</ul>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="machine-learning.html#cb99-1" tabindex="-1"></a>irisTest[, <span class="sc">-</span><span class="dv">1</span>]</span></code></pre></div>
<pre><code>##   Petal.Length Petal.Width
## 1          1.6         0.6
## 2          4.9         1.5</code></pre>
<ul>
<li><code>cl</code>: the class labels in the training data. These are in the <code>Species</code> column, so we can extract these using</li>
</ul>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="machine-learning.html#cb101-1" tabindex="-1"></a>irisTrain<span class="sc">$</span>Species</span></code></pre></div>
<pre><code>## [1] setosa     setosa     versicolor versicolor virginica  virginica 
## Levels: setosa versicolor virginica</code></pre>
<p>So, we use the <code>knn()</code> function as follows:</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="machine-learning.html#cb103-1" tabindex="-1"></a><span class="fu">library</span>(class)</span>
<span id="cb103-2"><a href="machine-learning.html#cb103-2" tabindex="-1"></a><span class="fu">knn</span>(<span class="at">train =</span> irisTrain[, <span class="sc">-</span><span class="dv">3</span>], </span>
<span id="cb103-3"><a href="machine-learning.html#cb103-3" tabindex="-1"></a>    <span class="at">test =</span> irisTest[, <span class="sc">-</span><span class="dv">1</span>],</span>
<span id="cb103-4"><a href="machine-learning.html#cb103-4" tabindex="-1"></a>    <span class="at">cl =</span> irisTrain<span class="sc">$</span>Species)</span></code></pre></div>
<pre><code>## [1] setosa     versicolor
## Levels: setosa versicolor virginica</code></pre>
<p>So, out of the three possible <code>Levels</code>, the KNN algorithm classifies flower <code>A</code> as <code>setosa</code> and flower <code>B</code> as versicolor. This agrees with what we could see in Figure <a href="machine-learning.html#fig:simpleIris">2.7</a></p>
</div>
<div id="using-knn-with-the-handwritten-digits" class="section level4 hasAnchor" number="2.1.5.2">
<h4><span class="header-section-number">2.1.5.2</span> Using <code>knn()</code> with the handwritten digits<a href="machine-learning.html#using-knn-with-the-handwritten-digits" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Returning to the handwritten digits, suppose we have the training images and class labels stored in a single data frame called <code>training_set</code>, where each row is one image, with the class label in column 1 and the pixel values in columns 2 to 785. We display the first 5 columns below:</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="machine-learning.html#cb105-1" tabindex="-1"></a>training_set[, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code></pre></div>
<pre><code>## # A tibble: 60,000 × 5
##       X1    X2    X3    X4    X5
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1     5     0     0     0     0
##  2     0     0     0     0     0
##  3     4     0     0     0     0
##  4     1     0     0     0     0
##  5     9     0     0     0     0
##  6     2     0     0     0     0
##  7     1     0     0     0     0
##  8     3     0     0     0     0
##  9     1     0     0     0     0
## 10     4     0     0     0     0
## # ℹ 59,990 more rows</code></pre>
<p>(Look at the first column, and compare it with the first row in Figure <a href="machine-learning.html#fig:MNIST">2.1</a>, which shows the first 10 images in the training data set.)</p>
<p>We now extract the class labels and images as follows:</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="machine-learning.html#cb107-1" tabindex="-1"></a>training_images <span class="ot">&lt;-</span> training_set <span class="sc">%&gt;%</span></span>
<span id="cb107-2"><a href="machine-learning.html#cb107-2" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>X1)</span></code></pre></div>
<p>The minus sign means that we select all columns <em>apart</em> from <code>X1</code>. We then extract the class labels:</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="machine-learning.html#cb108-1" tabindex="-1"></a>training_labels <span class="ot">&lt;-</span> training_set<span class="sc">$</span>X1</span></code></pre></div>
<p>Suppose the test set are arranged in another data frame <code>test_set</code>, with the same structure. We then extract the images:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="machine-learning.html#cb109-1" tabindex="-1"></a>test_images <span class="ot">&lt;-</span> test_set <span class="sc">%&gt;%</span></span>
<span id="cb109-2"><a href="machine-learning.html#cb109-2" tabindex="-1"></a>  <span class="fu">select</span>(<span class="sc">-</span>X1)</span></code></pre></div>
<p>Now we can use the <code>knn()</code> function. We’ll just use it on the first 5 images in the test set:</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="machine-learning.html#cb110-1" tabindex="-1"></a><span class="fu">library</span>(class)</span>
<span id="cb110-2"><a href="machine-learning.html#cb110-2" tabindex="-1"></a><span class="fu">knn</span>(<span class="at">train =</span> training_images,</span>
<span id="cb110-3"><a href="machine-learning.html#cb110-3" tabindex="-1"></a>    <span class="at">test =</span> test_images[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, ],</span>
<span id="cb110-4"><a href="machine-learning.html#cb110-4" tabindex="-1"></a>    <span class="at">cl =</span> training_labels)</span></code></pre></div>
<pre><code>## [1] 7 2 1 0 4
## Levels: 0 1 2 3 4 5 6 7 8 9</code></pre>
<p>(The first row of the output means that the algorithm estimates the first five digits to be 7, 2, 1, 0, 4. The second row gives the full range of digits provided in <code>training_labels</code>.)
Inspecting the first five test images, we can see that the algorithm has worked!</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-78"></span>
<img src="MAS109-Data-Science_files/figure-html/unnamed-chunk-78-1.png" alt="The first five images in the test set. We can see that algorithm has correctly identified all five digits." width="480" />
<p class="caption">
Figure 2.8: The first five images in the test set. We can see that algorithm has correctly identified all five digits.
</p>
</div>
</div>
</div>
<div id="the-performance-of-the-algorithm" class="section level3 hasAnchor" number="2.1.6">
<h3><span class="header-section-number">2.1.6</span> The performance of the algorithm<a href="machine-learning.html#the-performance-of-the-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The algorithm won’t always get it right! Here’s an example where the algorithm gets it wrong (test image number 116):</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="machine-learning.html#cb112-1" tabindex="-1"></a><span class="fu">knn</span>(<span class="at">train =</span> training_images,</span>
<span id="cb112-2"><a href="machine-learning.html#cb112-2" tabindex="-1"></a>    <span class="at">test =</span> test_images[<span class="dv">116</span>, ],</span>
<span id="cb112-3"><a href="machine-learning.html#cb112-3" tabindex="-1"></a>    <span class="at">cl =</span> training_labels)</span></code></pre></div>
<pre><code>## [1] 9
## Levels: 0 1 2 3 4 5 6 7 8 9</code></pre>
<p>After a little investigation (details omitted), it turns out that image 8112 in the training data is closest to image 116 in the test data. If we look at the image, we can see where the algorithm went wrong: the test image is a ‘4’, but it looks very similar to a ‘9’ in the training data.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-82"></span>
<img src="MAS109-Data-Science_files/figure-html/unnamed-chunk-82-1.png" alt="Top row: test image 116 and its numerical representation $\tilde{\mathbf x}_{116}$. Bottom row: image number 8112 in the training data set and its numerical representation $\mathbf x_{8112}$. This training image is  the closest to our test image, but the digits are not the same!" width="576" />
<p class="caption">
Figure 2.9: Top row: test image 116 and its numerical representation <span class="math inline">\(\tilde{\mathbf x}_{116}\)</span>. Bottom row: image number 8112 in the training data set and its numerical representation <span class="math inline">\(\mathbf x_{8112}\)</span>. This training image is the closest to our test image, but the digits are not the same!
</p>
</div>
<p>Applying the algorithm to all 10000 test images, the algorithm gets the right answer 9691 times (96.91%). This may look quite good, but an error rate of 3% is probably too large in practice. But the performance is good enough to show the potential of using a data-based method for the image recognition problem. In fact, more complex methods do give better performance. A list of results is maintained <a href="http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html">here</a>, with the best performing method (at the time of writing) being 99.79% accurate.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Samuel, Arthur (1959). <a href="https://ieeexplore.ieee.org/document/5392560">Some Studies in Machine Learning Using the Game of Checkers</a>. IBM Journal of Research and Development. 3 (3): 210-229.<a href="machine-learning.html#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="exploratory-data-analysis-using-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="populations-samples-and-statistical-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
