[["index.html", "Data Science R Practicals and Exercises Instructions R Packages R Markdown", " Data Science R Practicals and Exercises Dr Jill Johnson 2025-02-27 Instructions These pages contain R Practicals and exercises for students on MAS109. The Chapter exercises refer to the chapters in the lecture notes. R Packages To do the R practicals, you will need to install the R packages tidyverse and rmarkdown. Start RStudio and run the following command in the console window. install.packages(c(&quot;tidyverse&quot;, &quot;rmarkdown&quot;)) R may ask you the question: ## There are binary versions available but the source versions are later. ## Do you want to install from sources the package which needs compilation? You can answer n to this. R Markdown You will need to use R Markdown for the R Practicals. A short tutorial with videos on R Markdown is available here "],["r-practical-1-brexit.html", " 1 R Practical 1: Brexit 1.1 Introduction 1.2 Tasks 1.3 Data sources", " 1 R Practical 1: Brexit 1.1 Introduction The spreadsheet Brexit.csv, available on Blackboard, contains data from the 2016 referendum on the UK’s membership of the European Union. Each row represents one local authority district in the UK. The columns are as follows. Region: a geographical region of the UK in which the district is located; Area: the name of the local authority district; remain: the percentage of votes in the district to remain in the European Union; unemployed: the percentage of unemployed residents in the district, aged 16 to 74, in 2011. level4: the percentage of residents in the district, aged 16 to 74, with qualifications at level 4 and above in 2011. (An A-level is a level 3 qualification). medianage: the median age of residents in the district, in 2011. (To simplify this practical, we have omitted the data for Northern Ireland and Gibraltar. Their remain vote percentages were 55.78% and 95.51% respectively.) Your task is to investigate whether there is any relationship between the remain vote percentage in each district and the other variables. 1.2 Tasks If you haven’t already done so, create a folder for this module on your computer (or U: drive if on campus). Inside that folder, create another folder: Practical 1. Download the files Brexit.csv and Practical1.Rmd from Blackboard, and put them into your Practical 1 folder. Open the file Brexit.csv to inspect it. Open the R Markdown document Practical1.Rmd in RStudio. Change the author to your name Run the first code chunk to load the tidyverse package (click on the green arrow). Your solutions to the remaining tasks should all go in this R Markdown document, with one code chunk per task. Import the data Brexit.csv into R, storing it as a data frame called brexit. Inspect the first ten rows. Check this against the data in Excel. Copy and modify this lecture notes example: importing data Here, rather than maths.csv, you are importing a file called Brexit.csv. You need to store the result as brexit rather than maths. What were the lower quartile, median and upper quartile of percentage of remain voters in the 380 districts? You can get the median and quartiles using the summary() command. Here is an example. Find the percentage of the remain voters in Sheffield. (You will need to select the row from the Brexit data frame in which the Area column takes the value Sheffield). You could just search through the spreadsheet, but you should practise using the filter() command to select rows from a data frame. Here is an example. Find the districts with the highest 10 percentages of remain voters and lowest 10 percentages of remain votes. What do you notice about the regions? You can use the arrange() command to arrange the rows of the data frame in order of the remain variable. Here is an example. Produce three scatter plots, with the remain vote percentage on the \\(y\\)-axis in each plot, and each of the employed, level4, medianage variables on the \\(x\\)-axis. For each plot change the axes labels to make them more informative; display a linear trend on each scatterplot; use different colours of points to represent different regions; add a caption to your plot, which includes the value of Pearson’s correlation coefficient between the variables, and states what conclusion you would draw from the plot. Use a separate code chunk for each plot. To produce a basic scatter plot, look at the example here. To use different colours per region, and to add proper axes labels, see this example on customising a scatter plot. Here’s an example of adding a linear trend Use the cor() command to compute Pearson’s correlation. Here’s an example. To add the caption, you need to include an argument fig.cap = \"Put your caption here\" as a chunk option in your R Markdown document. Obtain the mean percentage of the remain voters in each Region, and arrange in order of the mean remain vote. You’ll need to make use of the group_by() and arrange() commands. Here is an example of computing summaries per group Make sure you understand how the “pipe operator” %&gt;% works for chaining commands together. Here is an example. Produce a web page that presents your solutions. Click on the Knit arrow, and choose the output type. To customise the appearance of your document, please refer to the tutorial notes and video on code chunk options. If you get an error, two possible problems are You have an install.packages() command somewhere in your R Markdown document: delete it. It’s best to run install.packages() commands in the console, as they only need to be run once. There is no command within your R Markdown document that loads the data (you imported the data some other way). 1.3 Data sources The data were obtained from (https://www.electoralcommission.org.uk/find-information-by-subject/elections-and-referendums/past-elections-and-referendums/eu-referendum) and (https://www.nomisweb.co.uk/) Age by district: (https://www.nomisweb.co.uk/query/construct/summary.asp?mode=construct&amp;version=0&amp;dataset=145) Economic activity: (https://www.nomisweb.co.uk/query/construct/summary.asp?mode=construct&amp;version=0&amp;dataset=1511) Education: (https://www.nomisweb.co.uk/query/construct/summary.asp?mode=construct&amp;version=0&amp;dataset=1510) All data accessed 12/10/17. "],["r-practical-2-mapping-house-price-variation-in-sheffield.html", " 2 R Practical 2: mapping house price variation in Sheffield 2.1 Introduction 2.2 Tasks 2.3 Data sources", " 2 R Practical 2: mapping house price variation in Sheffield To do this practical, you will need to install the leaflet package. Before you do anything else, start RStudio and run the command install.packages(&quot;leaflet&quot;) 2.1 Introduction The spreadsheet house.csv, available on Blackboard, contains data on house prices, for each house sold in Sheffield in 2016, for the postcode districts S1 to S11. Each row corresponds to one house sale. Download the file from Blackboard onto your computer. Open this file in Excel to inspect the data. The columns are as follows. -price: the price paid (in pounds) for the house; -Postcode: the postcode of the house; -Latitude and Longitude: the geographical coordinates of the postcode; -district: the postcode district of the house (the first letter and number of the full postcode). Your main task is to investigate how house prices vary geographically. 2.2 Tasks From the previous practical, you should already have a folder for this module on your computer (or U: drive). Inside that folder, create another folder: Practical 2. Download the files house.csv and Practical2.Rmd from Blackboard, and put them into your Practical 2 folder. Open the file house.csv in Excel to inspect it. Once the installation of the leaflet package has finished, open the rmarkdown document Practical2.Rmd in RStudio. Put your solutions to the following tasks in this rmarkdown document. (This animation is from R Practical 1, but you need to do the same sort of thing here. Change the title at the top, as well as the author name.) Import the data into R, storing it as a data frame called house. Inspect the first ten rows. Check these against the file in Excel, to make sure the data have been imported correctly. You need to import your data by using a suitable command inside your .Rmd document. Do not import your data any other way. Copy and modify this lecture notes example: importing data Here, rather than maths.csv, you are importing a file called house.csv. You need to store the result as house rather than maths. Use the summary command to obtain some basic summary statistics of house prices. You should see a noticeable difference between the mean and median. From the output of the summary command, what do you think has caused this? Which would be a better choice to indicate a `typical’ house price: the mean or the median? Here is an example of using the summary() command. Note that using this command involves extracting the values from a single column in your dataframe: a column called price from a dataframe called house: here is an example, where we extract the values from a column called score from a dataframe called maths. Produce a suitable plot for displaying the distribution of all house prices. Specify a label for the \\(x\\)-axis so that the units are included. How would you describe the shape of this distribution? Have a look at sections 1.6-1.8.1 in your notes. Produce a suitable plot to compare the distribution of house prices per postcode district. Which three districts appear to have the most variation in house prices? Have a look at section 1.11 in your notes. You will now investigate where the cheapest and most expensive houses tend to be. Find the 5th percentile and the 95th percentile of the house prices in the data set, and assign them to the variables p05 and p95. You will find the quantile() command helpful. Follow the instructions in Practical2.Rmd to produce a map of the most expensive and cheapest house prices. (You should be able to scroll and zoom on the map if you want.) What do you notice? (Any ideas as to why?) 2.3 Data sources The house price data from this practical were obtained from (https://www.gov.uk/government/statistical-data-sets/price-paid-data-downloads) Data produced by Land Registry (c) Crown copyright 2016. The postcode coordinates data were obtained from (https://www.doogal.co.uk/AdministrativeAreas.php?district=E08000019) Contains Ordnance Survey data (c) Crown copyright and database right 2017 Contains National Statistics data (c) Crown copyright and database right 2017 All data accessed 9/02/17. "],["r-practical-3-detecting-breast-cancer.html", " 3 R Practical 3: detecting breast cancer 3.1 Introduction 3.2 Data 3.3 Tasks 3.4 Data source", " 3 R Practical 3: detecting breast cancer 3.1 Introduction A study of breast cancer diagnosis (and prognosis) based on the work of Prof. Olvi L. Mangasarian and Dr. William H. Wolberg is described at (http://pages.cs.wisc.edu/~olvi/uwmp/cancer.html). The aim is to diagnose breast cancer based on a technique known as Fine Needle Aspiration (FNA). An FNA sample is taken from a breast mass, and then examined on a microscopic slide. Various characteristics of the cell nuclei are then recorded (30 in total). The aim is then to determine whether the breast mass is cancerous or not (“malignant” or “benign”), based on these measurements only. To design a diagnostic tool, a data set has been obtained of the 30 measurements for each patient, together with the true status of the breast mass (malignant or benign), obtained from a more invasive surgical procedure. Your task is then to investigate whether the true status can be determined from the 30 measurements only (which would mean patients could be diagnosed without surgery). 3.2 Data There are two data sets (on Blackboard) for this practical. The first, cancer-training.csv is known as the training set. You will use this dataset to work out how to use the measurements to detect breast cancer. Each row corresponds to one patient, and there are 32 columns: ID is a label for the patient (the patients used in this study are anonymous); status refers to whether the breast mass is malignant, (the patient has breast cancer), or benign (the patient does not have breast cancer); the remaining 30 columns are the 30 different measurements taken on the patient’s cell nuclei. These have all been normalised to be on the same scale of 0 to 1. For example, the patient with ID 842302 has a vector of 30 measurements \\[ x = (0.521,\\, 0.023,\\, 0.546,\\ldots,0.419) \\] describing her cell nuclei, with a corresponding class label of “malignant”. (We won’t worry with the definitions of these measurements in this practical1). The second, cancer-test.csv is known as the test set. In this data set, the 30 measurements are given for three patients, but you are not told whether each breast mass is malignant or not: your task is to predict this, given what you have learned from the training set. 3.3 Tasks Inside your folder for this module, create another folder: Practical 3. Download the two data sets cancer-training.csv and cancer-test.csv, and the file Practical3.Rmd (all on Blackboard) and put them in your Practical 3 folder. Import the files cancer-training.csv and cancer-test.csv into R, storing them under the names cancer.training and cancer.test respectively You need to import your data by using suitable commands inside your .Rmd document. Do not import your data any other way. Copy and modify this lecture notes example: importing data Here, rather than maths.csv, you are importing a file called cancer-training.csv. You need to store the result as cancer.training rather than maths. Repeat this process to get cancer.test. The xtabs() command can be used to make a table of frequencies for string (text) or factor data. For example: myVector &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;, &quot;b&quot;, &quot;b&quot;, &quot;b&quot;, &quot;a&quot;) xtabs( ~ myVector) ## myVector ## a b ## 3 5 Use this command to find out how many malignant cases and how many benign cases there are in the training data. See this example for extracting the values of a column from a data frame. Calculate a 95% confidence interval for the mean value of the smoothness.mean variable, assuming these observations are normally distributed. Treat the malignant and benign groups as a single group (in other words, ignore the status variable). See this example for extracting the values of a column from a data frame. The confidence interval formula is given here. Use the functions mean() and var() for computing the sample mean and the sample variance, and see here for working with the \\(t\\)-distribution in R. Use the \\(K\\)-nearest neighbour algorithm (with \\(K=1\\)) from Chapter 2 in your notes, to estimate whether each patient in the test set has breast cancer or not. Note: you can select all but the first two columns from cancer.training with the command cancer.training[, -(1:2)], and you can select all but the first column from cancer.test with the command cancer.test[, -1] You will now try some exploratory analysis to check your results. For the cancer.training data, produce a scatter plot of the variables symmetry.se and texture.se against each other. Colour the points according to whether the breast mass was classified as malignant or benign. Annotate your plot with the three patient labels in cancer.test data, plotted at the appropriate coordinates. Experiment with including the option alpha = 0.5 within your geom_point() command. The alpha value can be between 0 and 1 and controls the transparency of the points. This helps if points are being drawn over each other. Some code to help you do the annotation is in the file Practical3.Rmd See here for an example of drawing a scatter plot with colours to indicate groups. You will see that it is not easy to tell whether the breast mass is cancerous or not for the three test patients, using only the two measurements symmetry.se and texture.se. This is because the malignant and benign points are all `jumbled up’: the location of a point on the plot doesn’t depend on whether the breast mass is malignant or benign. A plot of the measurements looks something like this: We would like to identify two measurements where the separation into malignant and benign cases is clearer: something like the following. We can calculate how well a particular measurement separates into malignant and benign cases, by computing \\[(|\\bar{x}_m - \\bar{x}_b|)/(s_m + s_b)\\] for each measurement, with \\(\\bar{x}_m\\) and \\(s_m\\) the sample mean and sample standard deviation of the malignant cases for that measurement, and \\(\\bar{x}_m\\) and \\(s_m\\) the sample mean and sample standard deviation of the benign cases. Run the code provided in Practical3.Rmd to get the mean and standard deviation of the malignant observations, for each measurement. Copy and then modify the code to get the mean and standard deviation of the benign observations, for each measurement. Obtain the statistic \\((|\\bar{x}_m - \\bar{x}_b|)/(s_m + s_b)\\) for each measurement. Use the command abs() to obtain the absolute value. Store the result in a single vector called scaled.differences. Inspect the vector scaled.differences to see which measurements are most suitable for detecting breast cancer: use the command sort(scaled.differences, decreasing = FALSE) (You should find that symmetry.se and texture.se have the smallest values). Produce a scatter plot of your best two measurements against each other. As before: colour the points according to whether the breast mass was classified as malignant or benign. Annotate your plot with the three patient labels in cancer.test data, plotted at the appropriate coordinates. Experiment with including the option alpha = 0.5 within your geom_point() command. Now can you decide whether these three breast masses in your test set are malignant or benign? Compare the plot with your results from Task 6. 3.4 Data source The data were obtained from (ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/WDBC/WDBC.dat) [Accessed 4/01/18] You don’t need to, but if you want to find out what they are, they are described in the document at (ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/WDBC/WDBC.doc)↩︎ "],["r-practical-4-can-brain-training-games-make-you-smarter.html", " 4 R practical 4: can brain-training games make you smarter? 4.1 Introduction 4.2 Tasks 4.3 Data source", " 4 R practical 4: can brain-training games make you smarter? 4.1 Introduction Various games have been devised which, according to their designers, can boost your intelligence if you play them regularly. How can we tell if they work? In this practical, you will analyse data from an experiment reported in Lawlor-Savage and Goghari (2016)2, designed to test the effectiveness of dual-n-back games compared with “processing speed training”. In their experiment: 84 healthy adults, aged 30-60, were allocated at random to one of two games: dual-n-back, or processing speed training. (This is the control group. It is not believed “processing speed training” has any effect on intelligence.) 57 adults were included in the final analysis: these adults played their respective games on at least 13 occasions over a 5 week period, playing the game for around 30 minutes per occasion. The adults took various tests at the start (before playing any games) and end of the experiment. In this practical, you will analyse their scores from one test: Raven’s Advanced Progressive Matrices (a test of fluid intelligence). Try dual-n-back training for yourself, using the demo available here (you will need sound enabled). You will see a sequence: in each step, a square appears in one of the grid cells, and you will hear a letter. You have to remember what you saw and heard two steps back in the sequence Press the appropriate keys whenever the position of the square and/or the letter is repeated from two steps back. (It’s hard!) Data are in the file brain2.csv, available on Blackboard. The column headings are subject: a label for each participant; game: which game the participant trained with: dual-n-back, or speed processing training. RAPM1: the score on Raven’s Advanced Progressive Matrices (RAPM) before training; RAPM2: the score on Raven’s Advanced Progressive Matrices after training; RAPM.improve: the change between the two scores (RAPM after training - RAPM before training) 4.2 Tasks Inside your folder for this module, create another folder: Practical 4. Download the R Markdown document Practical4.Rmd and data brain2.csv from Blackboard, and put these files into your Practical 4 folder. You must put your solutions to the following tasks in the R Markdown document. (This animation is from R Practical 1, but you need to do the same sort of thing here. Change the title at the top, as well as the author name.) Import the data into R, storing it as a data frame called brain. You need to do this using a suitable command inside your .Rmd document. If you import your data some other way, although you will still be able to do tasks 4-7, your .Rmd document will not compile successfully. Copy and modify this lecture notes example: importing data Here, rather than maths.csv, you are importing a file called brain2.csv. You need to store the result as brain rather than maths. For all the participants in the study, find the mean and interquartile range of the RAPM scores taken at the start of the experiment. You can get various summary statistics using the summary() command. Here is an example. The interquartile range is defined here. Assuming RAPM scores are normally distributed, calculate a 95% confidence interval for the population mean RAPM score at the beginning of the experiment. (You can treat all the participants as a single sample, ignoring which game they played: none of the participants had played either game at the start of the experiment). See this example for extracting the values of a column from a data frame. The confidence interval formula is given here. Use the functions mean() and var() for computing the sample mean and the sample variance, and see here for working with the \\(t\\)-distribution in R. Conduct a suitable hypothesis test to test whether the dual-n-back game has improved intelligence, compared with speed processing training. Find the 95% confidence interval for the difference between mean improvements in the two groups To do a two-sample \\(t\\)-test in R, see Section 7.6.1. Using one of the types of plots described in Chapter 1, produce a suitable plot to compare the improvements in RAPM scores between the two groups. Specify axes labels to make them more descriptive; Add a caption, stating what the plots suggest regarding whether the dual-n-back game has improved intelligence. One of the plot types in Chapter 1 is described as being useful for comparing groups. To add a caption, use a code chunk option fig.cap = \"Insert your caption here\" 4.3 Data source The data were obtained from (https://figshare.com/articles/Working_Memory_Training_in_Healthy_Adults/3121582) [Accessed 20/12/17] Lawlor-Savage L, Goghari VM (2016) Dual N-Back Working Memory Training in Healthy Adults: A Randomized Comparison to Processing Speed Training. PLoS ONE 11(4): e0151817. (https://doi.org/10.1371/journal.pone.0151817)↩︎ "],["chapter-1-problems.html", " 5 Chapter 1 problems", " 5 Chapter 1 problems Annual salaries for 1000 full-time employees have been recorded. The data are displayed in a box plot below. The data are stored in R in a vector called salary. If the data were instead plotted with a histogram, sketch the approximate shape of the histogram. (Concentrate on the basic shape of the histogram: do not worry about the detail.) Using the following R output summary(salary) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 15166 19599 23063 24937 28960 63981 What value would best describe a `typical’ salary? Justify your answer. What does the output tell you about variation in salaries? ``Anscombe’s quartet’’ is a set of four data sets, constructed by the statistician Francis Anscombe (1918-2001), and shown below. ## x1 y1 x2 y2 x3 y3 x4 y4 ## [1,] 10 8.04 10 9.14 10 7.46 8 6.58 ## [2,] 8 6.95 8 8.14 8 6.77 8 5.76 ## [3,] 13 7.58 13 8.74 13 12.74 8 7.71 ## [4,] 9 8.81 9 8.77 9 7.11 8 8.84 ## [5,] 11 8.33 11 9.26 11 7.81 8 8.47 ## [6,] 14 9.96 14 8.10 14 8.84 8 7.04 ## [7,] 6 7.24 6 6.13 6 6.08 8 5.25 ## [8,] 4 4.26 4 3.10 4 5.39 19 12.50 ## [9,] 12 10.84 12 9.13 12 8.15 8 5.56 ## [10,] 7 4.82 7 7.26 7 6.42 8 7.91 ## [11,] 5 5.68 5 4.74 5 5.73 8 6.89 Each dataset is made up of 11 pairs \\((x_1,y_1),\\ldots,(x_{11},y_{11})\\) so, for example, for the first dataset, the observations are \\((10, 8.04),\\ (8, 6.95),\\ldots,(5, 5.68)\\). All four data sets have the same summary statistics (correct to 1 d.p.): \\[ \\sum_{i=1}^{11}x_i = 99,\\quad \\sum_{i=1}^{11}x_i^2 = 1001,\\quad \\sum_{i=1}^{11}y_i = 82.5,\\quad \\sum_{i=1}^{11}y_i^2 = 660.2,\\quad \\sum_{i=1}^{11}x_iy_i = 797.6. \\] Using the summary statistics, calculate Pearson’s correlation coefficient between the \\(x\\) and \\(y\\) observations in each dataset (it will be the same for all four data sets). Do you agree with the following statement? Each dataset has the same value of Pearson’s correlation coefficient, therefore, the relationship between the two variables must be the same in each dataset. If you want to see the four datasets, run this code in R: data(anscombe) x &lt;- c(anscombe$x1, anscombe$x2, anscombe$x3, anscombe$x4) y &lt;- c(anscombe$y1, anscombe$y2, anscombe$y3, anscombe$y4) df &lt;-data.frame(x=x, y =y, dataset = factor(rep(c(&quot;dataset 1&quot;, &quot;dataset 2&quot;, &quot;dataset 3&quot;, &quot;dataset 4&quot;), each = 11))) library(ggplot2) ggplot(df, aes(x=x, y=y))+ geom_point()+ facet_wrap(~dataset, ncol = 2) Challenge problem. Consider the PISA mathematics scores dataset in Chapter 1. If GDP per capita were instead measured in Euros rather than US$ (and assuming 1 Euro \\(\\neq\\) 1 dollar), prove that this would give a different value of the covariance between maths score and GDP per capita, but would not change the value of either Pearson’s or Spearman’s correlation. "],["chapters-3-and-4-problems.html", " 6 Chapters 3 and 4 problems", " 6 Chapters 3 and 4 problems An investigator is interested in the daily number of arrivals at a new hospital ward for premature babies. The hospital ward is not open yet, but will open shortly. Let \\(X_i\\) be the number of arrivals that there will be on day \\(i\\). She plans to observe the numbers of arrivals on each of the first 30 days, and so she will observe a sample \\(X_1,\\ldots,X_{30}\\). Suggest a suitable model for the data. Hint: choose one of the distributions used in the examples in Chapter 3. Present your answer by completing the following: \\[ X_1,\\ldots,X_{30} \\stackrel{i.i.d}{\\sim}\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_ \\] Your model in part (a) should have a single unknown parameter. The investigator plans to estimate this parameter using the estimator \\(\\bar{X}\\). Is this estimator unbiased and consistent? Following the notation in the lecture notes, explain the difference between \\(\\bar{X}\\) and \\(\\bar{x}\\) in this context. A dataset contains FEV1 (Forced Expiratory Volume 1) measurements (in litres) in healthy adults aged between 20-30 years. FEV1 is the volume of air forcefully exhaled in 1 second. (The ratio of FEV1 to FVC: forced vital capacity, the total volume of air that can be exhaled, can be used to detect lung disorders). There were 75 females and 75 males in the study, with their values stored in R in the vectors females and males respectively. Plots and summary statistics are shown below. ## Warning: package &#39;ggplot2&#39; was built under R version 4.4.2 c(sum(females), sum(females^2), sum(males), sum(males^2)) ## [1] 242.5690 786.8107 339.3340 1537.8311 Suggest a suitable model for these data, defining appropriate notation, and estimate the parameters. In a shop, times between customer arrivals are recorded, for 101 customers (e.g. customer number 17 arrived 30 seconds after customer number 16). A histogram of the data is shown below. What probability distribution would you use to model these data? The 100 times are stored in a vector times. Using the output below, suggest an estimate for the parameter in your distribution. sum(times) ## [1] 2061.353 Suppose we have random variables \\[ X_1,X_2,\\ldots,X_{n}\\stackrel{i.i.d}{\\sim} N(\\mu, \\sigma^2),\\] For the notation in Chapter 4, explain the difference between \\(\\mu\\), \\(\\bar{X}\\) and \\(\\bar{x}\\). Which of the following statements are correct? \\(\\mathbb{E}(X_i) = \\mu\\) \\(\\mathbb{E}(X_i) = \\bar{X}\\) \\(\\mathbb{E}(X_i) = \\bar{x}\\) \\(\\mathbb{E}(\\bar{X}) = \\mu\\) \\(\\mathbb{E}(\\bar{x}) = \\mu\\) \\(\\mathbb{E}(\\bar{x}) = \\mathbb{E}(\\bar{X})\\) For the notation in Chapter 4, explain the difference between \\(\\sigma^2\\), \\(S^2\\) and \\(s^2\\). Which of the following statements are correct? \\(\\mathbb{E}(S^2) = \\sigma^2\\) \\(\\mathbb{E}(s^2) = \\sigma^2\\) \\(\\mathbb{E}(s^2) = \\mathbb{E}(S^2)\\) \\(Var(X_i) = \\sigma^2\\) \\(Var(X_i) = s^2\\) \\(Var(X_i) = S^2\\) A random sample of 40 observations from a \\(N(\\mu, 4)\\) distribution provides the following information: \\(\\sum_{i=1}^{40}x_i = 334\\). Estimate \\(\\mu\\) and give the standard error of the estimate. How big would the sample have to be for the standard error to be no more than 0.1? How big would the sample have to be for the standard error to be no more than 0.01? Suppose \\(X_1,\\ldots,X_n\\) are independent and identically distributed random variables, each having the \\(N(0, \\sigma^2)\\) distribution. The situation here is a little different to that in Section 4.1.1 in your notes, in that the mean parameter \\(\\mu\\) is known to be 0. Prove that \\[ \\frac{1}{n}\\sum_{i=1}^nX_i^2 \\] is an unbiased estimator of \\(\\sigma^2\\). A survey polls a random sample of 300 students and finds that 120 of them are very satisfied with their accommodation. Estimate the proportion of the population (from which the sample was taken) that are very satisfied with their accommodation. Give the (estimated) standard error for this estimate. In the manufacture of hip replacement joints, a particular component has to be made to certain specifications: if its length, breadth and height do not lie within specified ranges, the component will not fit, and will have to be scrapped. A new process has been designed for the manufacture of this component, and the interest is in the population proportion, \\(\\theta\\) of components that will be compliant (and so do not need to be scrapped). A random sample of \\(n\\) components is to be selected, and the number \\(X\\) out of \\(n\\) components that are compliant will be observed. Choose a suitable probability distribution for \\(X\\), that links the observed proportion of compliant components to the population proportion \\(\\theta\\). Show that \\(\\frac{X}{n}\\) is an unbiased and consistent estimator for \\(\\theta\\). Two different measuring devices are used to measure the concentration of glucose in an individual’s blood. Neither device is perfectly accurate: there is likely to be an error in each measurement, but the (absolute value of the) error is likely to be smaller using the first device. Before the measurements are taken, let \\(X\\) and \\(Y\\) represent the two measurements, and let \\(\\mu\\) be the true glucose concentration (all in millimoles per litre). We model \\(X\\) and \\(Y\\) as independent normal random variables, each having expected value \\(\\mu\\) but having different variances \\(\\sigma^2\\) and \\(\\tau^2\\) respectively: \\[X\\sim N(\\mu, \\sigma^2),\\] \\[Y\\sim N(\\mu, \\tau^2).\\] If the linear combination \\(Z = aX+bY\\) is to be used as an estimator of \\(\\mu\\), explain what relationship between \\(a\\) and \\(b\\) is necessary for this estimator to be unbiased. Derive an expression for the standard error of \\(Z\\). Prove that the unbiased estimator of this form with smallest standard error has \\[\\begin{align*} a &amp;= \\frac{\\tau^2}{\\sigma^2+\\tau^2},\\\\ b &amp;= \\frac{\\sigma^2}{\\sigma^2+\\tau^2}, \\end{align*}\\] and variance \\[ \\frac{\\sigma^2\\tau^2}{\\sigma^2+\\tau^2}. \\] If the first device was known to be perfectly accurate, but the second device was not, what effect would this have on the expressions for \\(a\\) and \\(b\\) in part (c)? Give an intuitive (common sense) explanation for your result. Let \\(X_1,\\ldots,X_n\\) be independent and identically distributed random variables, with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Their distribution is not specified (i.e. in this question we do not assume they are normally distributed.) Obtain expressions for \\(\\mathbb{E}(X_i^2)\\) and \\(\\mathbb{E}(\\bar{X}^2)\\) in terms of \\(\\mu\\) and \\(\\sigma^2\\). Using the result that \\[ \\sum_{i=1}^n(X_i - \\bar{X})^2 = \\left(\\sum_{i=1}^nX_i^2 \\right)- n\\bar{X}^2, \\] obtain an expression for \\(\\mathbb{E}\\left(\\sum_{i=1}^n(X_i - \\bar{X})^2\\right)\\). Hence show that \\[ S^2:=\\frac{1}{n-1}\\sum_{i=1}^n(X_i - \\bar{X})^2 \\] is an unbiased estimator for \\(\\sigma^2\\). (This shows that \\(S^2\\) is an unbiased estimator of \\(Var(X_i)\\) for any distribution with a finite variance, given i.i.d. observations \\(X_1,\\ldots,X_n\\)). Challenge problem. Prove Theorem 4.1 in your notes: if an estimator is unbiased, and its standard error tends to 0 as the sample size \\(n\\) tends to infinity, it will also be a consistent estimator. Hint: use Chebyshev’s inequality. "],["chapter-5-problems.html", " 7 Chapter 5 problems", " 7 Chapter 5 problems Suppose \\(X_1,\\ldots,X_{10}\\) are independent and identically distributed variables, each having the \\(N(\\mu, \\sigma^2)\\) distribution. Denote the corresponding observed values by \\(x_1,\\ldots,x_{10}\\). Given the following summary statistics and using some of the R output below, calculate a 90% confidence interval for \\(\\mu\\). \\[ \\sum_{i=1}^{10}x_i = 47.8,\\quad\\sum_{i=1}^{10}x_i^2 = 232.5816. \\] qt(c(0.9, 0.95, 0.975), 9) ## [1] 1.383029 1.833113 2.262157 The following data are measurements of plasma citrate concentrations from 9 volunteers (in micromoles per litre): ## [1] 93 116 125 144 105 89 116 151 137 Assume that the population of such measurements is normally distributed. The observations are stored in a vector called plasma. Calculate a 95% confidence interval for the population mean and population variance based on these data, using (some of) the R output below. sum(plasma) ## [1] 1076 sum(plasma^2) ## [1] 132438 qt(c(0.025, 0.05, 0.95, 0.975), 8) ## [1] -2.3060 -1.8595 1.8595 2.3060 qchisq(c(0.025, 0.975), 8) ## [1] 2.1797 17.5345 A biased coin is tossed 50 times, and 32 heads are observed. Calculate an approximate 95% confidence interval for the probability a coin toss will result in a heads. An election is to be held between two candidates, and an opinion poll has been taken to see what percentage of the electorate will vote for candidate A. Assume the sample is representative, the respondents answer truthfully, and no-one is intending to change their minds; the only concern regarding the reliability of the poll is the sample size. If 60% answered that they intended to vote for candidate A, and 40% answered that they intended to vote for candidate B, how large would the sample size need to be to persuade you the poll would reliably predict the result? Some R output to help you is below. qnorm(c(0.8, 0.9, 0.95, 0.99)) ## [1] 0.84162 1.28155 1.64485 2.32635 Suppose \\(X\\sim N(\\beta, 1)\\), with \\(\\beta\\) unknown. It is intended to use the observed value of \\(X\\) to provide an interval estimate for \\(\\beta\\), where the interval will be given by \\([X-1.96,\\quad X+1.96]\\) Sketch the \\(N(\\beta,1)\\) density function, and indicate \\(\\beta\\) on the \\(x\\)-axis Indicate on your sketch (on the \\(x\\)-axis) what possible values \\(X\\) could take, such that \\([X-1.96,\\quad X+1.96]\\) would contain \\(\\beta\\). Using the fact that the 97.5th percentile of the standard normal distribution is 1.96, what is the probability that the random interval \\([X-1.96,\\quad X+1.96]\\) will contain \\(\\beta\\)? Fill in the missing value in the following statement. \\([X-1.96,\\quad X+1.96]\\) is a …..% confidence interval for \\(\\beta.\\) New cars from a factory are tested for their CO\\(_2\\) emissions, measured in grammes per kilometre travelled. The tests are conducted in a laboratory, under carefully controlled conditions. Twenty cars in total are tested: ten each of two models of car, A and B. For both models, the manufacturer has set a CO\\(_2\\) emissions target of 145,g/km. There is particular interest in whether the emissions will exceed 150 g/km: this would increase the tax that needs to be paid when the car is registered. Define \\(\\mu_A\\) and \\(\\mu_B\\) to be the population mean emissions respectively for all cars of model A and B that the factory produces. Denote the two corresponding sample means by \\(\\bar{x}_A\\) and \\(\\bar{x}_B\\). Data for the ten cars of each model are stored as vectors A and B in R. Given the following R output, write down the value of \\(\\bar{x}_A\\). Given that \\(\\bar{x}_A&lt;150\\),g/km, why would it be wrong to conclude that \\(\\mu_A\\) must be less than 150,g/km? sum(A) ## [1] 1483 sum((A - mean(A))^2) ## [1] 8.7314 By coincidence, the two samples have the same mean, but different variances: sum(B) ## [1] 1483 sum((B - mean(B))^2) ## [1] 113.03 The data are displayed in a plot (without a \\(y\\)-axis) below. You are asked to assess whether, for each model of car, the population mean emission value could be more than 150,g/km. By considering the plot, explain what is wrong with the following conclusion: Because \\(\\bar{x}_A = \\bar{x}_B\\), there is no reason to think \\(\\mu_B &gt; 150\\)g/km is more plausible than \\(\\mu_A &gt; 150\\)g/km. Assuming the observations are normally distributed, calculate 99% confidence intervals for \\(\\mu_A\\) and \\(\\mu_B\\), using the following R output. What conclusions would you draw about whether \\(\\mu_A &gt; 150\\)g/km and/or \\(\\mu_B &gt; 150\\)g/km? qt(0.995, 9) ## [1] 3.2498 Using the following R output, calculate an approximate 95% confidence interval for the proportion of cars of model B that have emissions exceeding 150g/km. Why is this interval “approximate”? sum(B &gt; 150) ## [1] 4 qnorm(0.975) ## [1] 1.96 If we wanted the confidence interval to have width of no more than 0.1, how many cars would be needed? Hints: Suppose in a future sample of \\(n\\) cars, we observe \\(x\\) cars to have emissions exceeding 150g/km. If we have \\(n=100\\) and \\(x=35\\), how wide would the confidence interval be? Now suppose, with \\(n=100\\), you didn’t know the corresponding value of \\(x\\). Would you still know the width of the confidence interval? Can you work out the widest possible width the interval could be? Now for any sample size \\(n\\), consider what the widest possible confidence interval could be. Challenge problem. A random observation \\(X\\) is to be drawn from the \\(U[0,\\theta]\\) distribution, with \\(\\theta\\) unknown. A one-sided 95% confidence interval for \\(\\theta\\) is to be constructed, of the form \\([kX,\\infty)\\), with \\(k\\) a suitably chosen constant. (This is called a one-sided confidence interval, as we are only attempting to provide a lower limit for \\(\\theta\\)). Derive the value of \\(k\\). "],["chapters-7-and-8-problems.html", " 8 Chapters 7 and 8 problems", " 8 Chapters 7 and 8 problems Baumann and Jones of the Purdue University Education Department conducted an experiment to compare different methods for reaching reading comprehension in children. Three methods were compared, but here we will consider two only: Basal - using textbooks designed to develop reading skills Directed Reading Thinking Activity (DRTA) - a comprehension strategy that guides students in asking questions about a text, making predictions, and then reading to confirm or refute their predictions. Twenty-two children were randomly allocated to each of these methods (so that we might expect similar abilities between the two groups at the start), and their reading comprehension was tested before and after instruction. Here, we just analyse scores after instruction. Histograms of the data are shown in the next plot. Figure 8.1: Histograms of reading comprehension scores for children taught by the basal and DRTA methods. From the data, we want to determine which method is best on average. The scores are stored in the R vectors basal and drta. Some R output is given below. basal ## [1] 5 9 5 8 10 9 12 5 8 7 12 4 4 8 6 9 3 5 4 2 5 7 drta ## [1] 7 5 13 5 14 14 10 13 12 11 8 7 10 8 8 10 12 10 11 7 8 12 c(mean(basal), var(basal), mean(drta), var(drta)) ## [1] 6.682 7.656 9.773 7.422 Write down a suitable model for the data, defining your notation carefully. You can assume test scores are normally distributed. State appropriate null and alternative hypotheses, in terms of your model parameters, to test whether the mean scores are different between the two methods. For an appropriate two-sample \\(t\\)-test, calculate the value of the test statistic Draw a sketch to illustrate what the \\(p\\)-value would represent, indicating the value of your test statistic. Give the R command needed to find the \\(p\\)-value. How would you interpret a \\(p\\)-value less than 0.001, with reference to the two teaching methods? A new 12 week weight-loss plan has been designed to help adults with a body mass index (BMI) \\(&gt;\\) 25 lose weight. Forty volunteers are recruited to a study to test the plan. Half the volunteers are assigned to the new plan, and the other half are assigned to an alternative plan: one that is currently recommended. The change in weight (in kg) is measured for each adult at the end of the study. The interest is in whether the new plan is any more effective than the current one. Assume the changes in weight in the two groups are normally distributed. Defining your notation carefully, write down a null and alternative hypotheses that would be appropriate for testing whether the new plan is more or less effective than the current one. Histograms for the two groups are shown below. The sample mean is indicated by the dashed line in each case. Note that the sample mean was lower (more weight lost on average) in the group with the new plan. If your hypothesis test in part (a) was to be conducted, at the 5% level of significance (so that you are using the Neyman-Pearson framework), by studying the plot below, what do you think the result would be? Briefly explain your answer. The changes in weight for the two groups are stored in R as the vectors newplan and currentplan respectively. Using the following R output, test your hypothesis in part a at the 5% level of significance. Assume that for \\(\\nu &gt;30\\), you can approximate the \\(t_\\nu\\) distribution by the \\(N(0,1)\\) distribution. State clearly the conclusion of your test, in terms of whether there is evidence from the experiment that the new plan is more effective than the current plan. c(mean(newplan), var(newplan)) ## [1] -0.6450 0.7089 c(mean(currentplan), var(currentplan)) ## [1] -0.285 0.515 qnorm(0.975) ## [1] 1.96 Using (some of) the following R output, deduce an interval within which the \\(p\\)-value from your hypothesis test must lie. qnorm(c(0.9, 0.95, 0.975)) ## [1] 1.282 1.645 1.960 Calculate a 95% confidence interval for the difference between the two population means, again, assuming that for \\(\\nu &gt;30\\), you can approximate the \\(t_\\nu\\) distribution by the \\(N(0,1)\\) distribution. By inspecting your interval, what would be the outcome of a test of the null hypothesis, of size 0.05, that patients on the new plan lose, on average 0.5kg more than patients on the current plan? Would this contradict your earlier conclusion? Two driving instructors are to be compared regarding how many of their pupils pass their driving tests at the first attempt. Instructor A has 30 pupils, and 18 pass at the first attempt. Instructor B has 35 pupils, and 12 pass at the first attempt. Conduct a suitable hypothesis to compare the two instructors. State your model for the data and define your notation carefully. Draw a sketch to indicate your observed test statistic and \\(p\\)-value. State the R command you would use to get the \\(p\\)-value. How would a \\(p\\)-value of 0.04 be interpreted in this case? Suppose we have two binomial random variables \\[\\begin{align*} X&amp;\\sim Bin(n, \\theta_X),\\\\ Y&amp;\\sim Bin(m, \\theta_Y), \\end{align*}\\] and we wish to test the null hypothesis \\[ H_0: \\theta_X = \\theta_Y. \\] We use the test statistic \\[ Z:= \\frac{\\frac{X}{n} - \\frac{Y}{m}}{\\sqrt{v}} \\] where \\[ v:= p^*(1-p^*)\\left(\\frac{1}{n} + \\frac{1}{m}\\right), \\] and \\[ p^*: = \\frac{x+y}{n+m}. \\] If we assume \\(H_0\\) is true, and we further assume \\(\\theta_X = \\theta_Y = \\frac{x+y}{n+m}\\), then prove that \\[\\begin{align} \\mathbb{E}(Z) &amp;= 0,\\\\ Var(Z)&amp;=1. \\end{align}\\] Challenge problem. In an investigation into the effects of a pesticide on the leaves of a plant, eight leaves were chosen and each was divided into two parts. One part of each leaf was treated with the pesticide and the other was not. Microscopic examination of the leaves for structural damage gave the following comparative responses, where a higher number corresponds to more damage. A scatter plot of these data are shown below. The dashed line has gradient 1 and \\(y\\)-intercept 0. Suppose the observations are normally distributed, and consider the following R output untreated &lt;- c(2.7, 4.6, 3.8, 4.8, 3.9, 6.2, 4.8, 3.1) treated &lt;- c(3.9, 4.3, 4.2, 5.4, 4.8, 6.1, 5.6, 3.6) differences &lt;- untreated - treated t.test(untreated, treated) ## ## Welch Two Sample t-test ## ## data: untreated and treated ## t = -1, df = 13, p-value = 0.3 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.5809 0.5809 ## sample estimates: ## mean of x mean of y ## 4.237 4.737 t.test(differences) ## ## One Sample t-test ## ## data: differences ## t = -2.8, df = 7, p-value = 0.03 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -0.9192 -0.0808 ## sample estimates: ## mean of x ## -0.5 In the second \\(t\\)-test, a one-sample \\(t\\)-test has been conducted. For a one-sample \\(t\\) test, we have random variables \\[ X_1,\\ldots,X_n\\stackrel{i.i.d}{\\sim}N(\\mu, \\sigma^2), \\] we test the hypothesis \\(H_0:\\mu = 0\\), and our test statistic is \\[ T = \\frac{\\bar{X}}{\\sqrt{S^2/n}}\\sim t_{n-1}, \\] assuming \\(H_0\\) is true. Do the two \\(p\\)-values give contradictory conclusions? If not, why not? "],["chapters-9-and-10-problems.html", " 9 Chapters 9 and 10 problems", " 9 Chapters 9 and 10 problems An experiment is planned to test a new treatment to help people to quit smoking. Volunteer smokers (who wish to quit) will be randomly allocated to one of two groups. In group 1, the volunteers will use the new treatment for two months. They will be recorded as “quitting” if they do not smoke at all for a six month period following treatment. The second group will act as a control group, in which the volunteers will instead be given a placebo, but then monitored in the same way. A total of \\(2n\\) volunteers will be recruited, with \\(n\\) volunteers allocated to each group. The data (the proportions quitting in each group) will be analysed with a Neyman-Pearson hypothesis test, with size 0.05. The investigators want to detect a difference of at least 10% in the proportions quitting in the two groups. They suggest assuming that 55% will quit in the treatment group, and 45% will quit in the control group. The investigators want their study to have a power of 80%. What sample size \\(n\\) per group would you recommend? Some R output to help you is below. qnorm(c(0.8, 0.975)) ## [1] 0.8416 1.9600 A random sample of 993 English voters were asked to name their favoured party. Each voter was also asked if they had attended or were currently attending university. The following table shows the results of cross-tabulating university education and political preferences of the voters. Con Lab Lib Other University: no 226 214 45 80 University: yes 151 185 42 50 Test the hypothesis that voting preferences are the same regardless of university education. Some R output to help is as follows. qchisq(c(0.95, 0.99, 0.999), 3) ## [1] 7.815 11.345 16.266 In a survey of 237 (Statistics) students from the University of Adelaide, amongst other variables, smoking habits (recorded here as one of Never\",Occasional”, Regular\" orHeavy”) and exercise levels (recorded here as one of Regular'' orsome/none’’) were observed. A contingency table is given below, with smoking status in the rows, and exercise in the columns. ## Regular Some/none ## Never 87 102 ## Occasional 12 7 ## Regular 9 8 ## Heavy 7 4 Test the hypothesis that exercise is independent of smoking status. Some R output to help is as follows. qchisq(c(0.95, 0.99, 0.999), 3) ## [1] 7.815 11.345 16.266 Challenge problem. A study compared two treatments A and B for kidney stones. For each treatment, the number of times the treatment was successful out of the total number of patients treated is given below. Treatment A Treatment B 273/350 289/350 Just by looking at the numbers, which treatment appears to be the most effective? It was also recorded whether each stone was ‘small’ or ‘large’: Treatment A Treatment B small 81/87 234/270 large 192/263 55/80 Considering small stones and large stones separately, and again, just by looking at the numbers, which treatment appears to be the most effective? Why is your conclusion different from that in part (a)? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
